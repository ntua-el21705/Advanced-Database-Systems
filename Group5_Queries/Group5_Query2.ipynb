{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0685c620",
   "metadata": {},
   "source": [
    "# Ζητούμενο 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4167cc",
   "metadata": {},
   "source": [
    "### α) Να υλοποιηθεί το Query 2 χρησιμοποιώντας τα DataFrame και SQL APIs. Να αναφέρετε και να συγκρίνετε τους χρόνους εκτέλεσης μεταξύ των δύο υλοποιήσεων. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b1e8e",
   "metadata": {},
   "source": [
    "#### Προεπεξεργασία για το query 2: Ενώνουμε τα 2 μέρη του κυρίως data set (Los Angeles Crime Data) σε ένα ενιαίο dataframe και στην συνέχεια το αποθηκεύουμε σε ένα μοναδικό .csv αρχείο στο S3 bucket του group 5, έτσι ώστε η μετέπειτα σύγκριση να γίνει υπό δίκαιες συνθήκες!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16fd658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>253</td><td>application_1738075734771_0254</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0254/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0254_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 1 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "crime_data_schema = StructType([StructField(\"dr_no\", IntegerType(), False),\n",
    "    StructField(\"date_rptd\", StringType(), True),\n",
    "    StructField(\"date_occ\", StringType(), True),\n",
    "    StructField(\"time_occ\", IntegerType(), True),\n",
    "    StructField(\"area\", IntegerType(), True),\n",
    "    StructField(\"area_name\", StringType(), True),\n",
    "    StructField(\"rpt_dist_no\", IntegerType(), True),\n",
    "    StructField(\"part_1_2\", IntegerType(), True),\n",
    "    StructField(\"crm_cd\", IntegerType(), True),\n",
    "    StructField(\"crm_cd_desc\", StringType(), True),\n",
    "    StructField(\"mocodes\", StringType(), True),\n",
    "    StructField(\"vict_age\", IntegerType(), True),\n",
    "    StructField(\"vict_sex\", StringType(), True),\n",
    "    StructField(\"vict_descent\", StringType(), True),\n",
    "    StructField(\"premis_cd\", IntegerType(), True),\n",
    "    StructField(\"premis_desc\", StringType(), True),\n",
    "    StructField(\"weapon_used_cd\", IntegerType(), True),\n",
    "    StructField(\"weapon_desc\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"status_desc\", StringType(), True),\n",
    "    StructField(\"crm_cd_1\", IntegerType(), True),\n",
    "    StructField(\"crm_cd_2\", IntegerType(), True),\n",
    "    StructField(\"crm_cd_3\", IntegerType(), True),\n",
    "    StructField(\"crm_cd_4\", IntegerType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"cross_street\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True)])\n",
    "\n",
    "\n",
    "crime_data_10_to_19_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crime_data_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "crime_data_20_to_present_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crime_data_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "\n",
    "unified_crime_data_df = crime_data_10_to_19_df.union(crime_data_20_to_present_df)\n",
    "\n",
    "group_number = \"5\"\n",
    "base_s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/CrimeData/\"\n",
    "csv_s3_path = base_s3_path + \"csv/\"\n",
    "unified_crime_data_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24860a7f",
   "metadata": {},
   "source": [
    "#### Εκτέλεση του query 2, χρησιμοποιώντας DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a467c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|precinct   |closed_case_rate  |rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.84713448949121 |1   |\n",
      "|2010|Olympic    |31.515289821999087|2   |\n",
      "|2010|Harbor     |29.36028339237341 |3   |\n",
      "|2011|Olympic    |35.040060090135206|1   |\n",
      "|2011|Rampart    |32.4964471814306  |2   |\n",
      "|2011|Harbor     |28.51336246316431 |3   |\n",
      "|2012|Olympic    |34.29708533302119 |1   |\n",
      "|2012|Rampart    |32.46000463714352 |2   |\n",
      "|2012|Harbor     |29.509585848956675|3   |\n",
      "|2013|Olympic    |33.58217940999398 |1   |\n",
      "|2013|Rampart    |32.1060382916053  |2   |\n",
      "|2013|Harbor     |29.723638951488557|3   |\n",
      "|2014|Van Nuys   |32.0215235281705  |1   |\n",
      "|2014|West Valley|31.49754809505847 |2   |\n",
      "|2014|Mission    |31.224939855653567|3   |\n",
      "|2015|Van Nuys   |32.265140677157845|1   |\n",
      "|2015|Mission    |30.463762673676303|2   |\n",
      "|2015|Foothill   |30.353001803658852|3   |\n",
      "|2016|Van Nuys   |32.194518462124094|1   |\n",
      "|2016|West Valley|31.40146437042384 |2   |\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 16.41 seconds"
     ]
    }
   ],
   "source": [
    "# DF query 2 execution\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType, StringType\n",
    "from pyspark.sql.functions import col, substring, count, round, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 2 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "unified_crime_data_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crime_data_schema) \\\n",
    "    .load(csv_s3_path)\n",
    "\n",
    "cases_with_year_df = unified_crime_data_df.withColumn(\"year\", substring(col(\"date_occ\"), 7, 4))\n",
    "\n",
    "total_cases_by_station_year_df = cases_with_year_df.groupBy(\"year\", \"area_name\") \\\n",
    "    .agg(count(\"*\").alias(\"total_cases\"))\n",
    "\n",
    "#total_cases_by_station_year_df.show(22)\n",
    "\n",
    "cases_closed_df = cases_with_year_df.filter(\n",
    "    (~col(\"status_desc\").contains(\"UNK\")) & \n",
    "    (~col(\"status_desc\").contains(\"Invest Cont\")))\n",
    "\n",
    "#cases_closed_df.show(5)\n",
    "\n",
    "cases_closed_by_station_year_df = cases_closed_df.groupBy(\"year\", \"area_name\") \\\n",
    "    .agg(count(\"*\") \\\n",
    "    .alias(\"closed_cases\"))\n",
    "\n",
    "#cases_closed_by_station_year_df.show(20)\n",
    "\n",
    "cases_with_percentages_df = cases_closed_by_station_year_df.join(total_cases_by_station_year_df, [\"year\", \"area_name\"]) \\\n",
    "    .withColumn(\"closed_case_rate\",(col(\"closed_cases\") / col(\"total_cases\") * 100))\n",
    "\n",
    "#cases_with_percentages.show(5)\n",
    "\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(col(\"closed_case_rate\").desc())\n",
    "\n",
    "query2_results_df = cases_with_percentages_df.withColumn(\"rank\", dense_rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 3) \\\n",
    "    .select(\"year\", col(\"area_name\").alias(\"precinct\"), \"closed_case_rate\", \"rank\")\n",
    "\n",
    "query2_results_df = query2_results_df.orderBy(\"year\", \"rank\")\n",
    "\n",
    "query2_results_df.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086b688",
   "metadata": {},
   "source": [
    "#### Εκτέλεση του query 2, χρησιμοποιώντας SQL APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509f9731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|precinct   |closed_case_rate  |rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.84713448949121 |1   |\n",
      "|2010|Olympic    |31.515289821999087|2   |\n",
      "|2010|Harbor     |29.36028339237341 |3   |\n",
      "|2011|Olympic    |35.040060090135206|1   |\n",
      "|2011|Rampart    |32.4964471814306  |2   |\n",
      "|2011|Harbor     |28.51336246316431 |3   |\n",
      "|2012|Olympic    |34.29708533302119 |1   |\n",
      "|2012|Rampart    |32.46000463714352 |2   |\n",
      "|2012|Harbor     |29.509585848956675|3   |\n",
      "|2013|Olympic    |33.58217940999398 |1   |\n",
      "|2013|Rampart    |32.1060382916053  |2   |\n",
      "|2013|Harbor     |29.723638951488557|3   |\n",
      "|2014|Van Nuys   |32.0215235281705  |1   |\n",
      "|2014|West Valley|31.49754809505847 |2   |\n",
      "|2014|Mission    |31.224939855653567|3   |\n",
      "|2015|Van Nuys   |32.265140677157845|1   |\n",
      "|2015|Mission    |30.463762673676303|2   |\n",
      "|2015|Foothill   |30.353001803658852|3   |\n",
      "|2016|Van Nuys   |32.194518462124094|1   |\n",
      "|2016|West Valley|31.40146437042384 |2   |\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 15.33 seconds"
     ]
    }
   ],
   "source": [
    "# SQL API query 2 execution\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType, StringType\n",
    "from pyspark.sql.functions import col, substring, count, round, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SQL API query 2 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "unified_crime_data_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crime_data_schema) \\\n",
    "    .load(csv_s3_path)\n",
    "\n",
    "unified_crime_data_df.createOrReplaceTempView(\"unified_crime_data\")\n",
    "\n",
    "# Create a new table with the 'year' column extracted\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW cases_with_year AS\n",
    "    SELECT *, SUBSTRING(date_occ, 7, 4) AS year FROM unified_crime_data\n",
    "\"\"\")\n",
    "\n",
    "# Combined query to compute closed case rates and rank top 3 areas per year\n",
    "query2_results_sql = spark.sql(\"\"\"\n",
    "    WITH \n",
    "    total_cases_by_station_year AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            area_name,\n",
    "            COUNT(*) AS total_cases\n",
    "        FROM cases_with_year\n",
    "        GROUP BY year, area_name\n",
    "    ),\n",
    "    cases_closed_by_station_year AS (\n",
    "        SELECT\n",
    "            year,\n",
    "            area_name,\n",
    "            COUNT(*) AS closed_cases\n",
    "        FROM cases_with_year\n",
    "        WHERE status_desc NOT LIKE '%UNK%'\n",
    "          AND status_desc NOT LIKE '%Invest Cont%'\n",
    "        GROUP BY year, area_name\n",
    "    ),\n",
    "    cases_with_percentages AS (\n",
    "        SELECT\n",
    "            c.year,\n",
    "            c.area_name,\n",
    "            c.closed_cases,\n",
    "            t.total_cases,\n",
    "            (c.closed_cases / t.total_cases * 100) AS closed_case_rate\n",
    "        FROM cases_closed_by_station_year c\n",
    "        JOIN total_cases_by_station_year t\n",
    "        ON c.year = t.year AND c.area_name = t.area_name\n",
    "    )\n",
    "    SELECT \n",
    "        year,\n",
    "        area_name AS precinct,\n",
    "        closed_case_rate,\n",
    "        rank\n",
    "    FROM (\n",
    "        SELECT \n",
    "            year,\n",
    "            area_name,\n",
    "            closed_case_rate,\n",
    "            DENSE_RANK() OVER (PARTITION BY year ORDER BY closed_case_rate DESC) AS rank\n",
    "        FROM cases_with_percentages\n",
    "    ) ranked\n",
    "    WHERE rank <= 3\n",
    "    ORDER BY year, rank\n",
    "\"\"\")\n",
    "\n",
    "query2_results_sql.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe99c7",
   "metadata": {},
   "source": [
    "### β) Να γράψετε κώδικα Spark που μετατρέπει το κυρίως data set σε parquet file format και αποθηκεύει ένα μοναδικό .parquet αρχείο στο S3 bucket της ομάδας σας. Επιλέξτε μία από τις δύο υλοποιήσεις του υποερωτήματος α) (DataFrame ή SQL) και συγκρίνετε τους χρόνους εκτέλεσης της εφαρμογής σας όταν τα δεδομένα εισάγονται σαν .csv και σαν .parquet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ba2af",
   "metadata": {},
   "source": [
    "#### Μετατροπή του κυρίως data set σε parquet file format και αποθήκευσή του σε ένα μοναδικό .parquet αρχείο στο S3 bucket του group 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97d019a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_number = \"5\"\n",
    "base_s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/CrimeData/\"\n",
    "\n",
    "parquet_s3_path = base_s3_path + \"parquet/\"\n",
    "unified_crime_data_df.coalesce(1).write.mode(\"overwrite\").parquet(parquet_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff08c48",
   "metadata": {},
   "source": [
    "#### Επιλέγουμε την υλοποίηση με DataFrame και εκτελούμε την εφαρμογή εισάγοντας τα δεδομένα σαν .parquet ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1cf9447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|precinct   |closed_case_rate  |rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.84713448949121 |1   |\n",
      "|2010|Olympic    |31.515289821999087|2   |\n",
      "|2010|Harbor     |29.36028339237341 |3   |\n",
      "|2011|Olympic    |35.040060090135206|1   |\n",
      "|2011|Rampart    |32.4964471814306  |2   |\n",
      "|2011|Harbor     |28.51336246316431 |3   |\n",
      "|2012|Olympic    |34.29708533302119 |1   |\n",
      "|2012|Rampart    |32.46000463714352 |2   |\n",
      "|2012|Harbor     |29.509585848956675|3   |\n",
      "|2013|Olympic    |33.58217940999398 |1   |\n",
      "|2013|Rampart    |32.1060382916053  |2   |\n",
      "|2013|Harbor     |29.723638951488557|3   |\n",
      "|2014|Van Nuys   |32.0215235281705  |1   |\n",
      "|2014|West Valley|31.49754809505847 |2   |\n",
      "|2014|Mission    |31.224939855653567|3   |\n",
      "|2015|Van Nuys   |32.265140677157845|1   |\n",
      "|2015|Mission    |30.463762673676303|2   |\n",
      "|2015|Foothill   |30.353001803658852|3   |\n",
      "|2016|Van Nuys   |32.194518462124094|1   |\n",
      "|2016|West Valley|31.40146437042384 |2   |\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 11.52 seconds"
     ]
    }
   ],
   "source": [
    "# DF query 2 execution and .parquet\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType, StringType\n",
    "from pyspark.sql.functions import col, substring, count, round, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 2 execution and .parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "unified_crime_data_df = spark.read.parquet(parquet_s3_path)\n",
    "\n",
    "cases_with_year_df = unified_crime_data_df.withColumn(\"year\", substring(col(\"date_occ\"), 7, 4))\n",
    "\n",
    "total_cases_by_station_year_df = cases_with_year_df.groupBy(\"year\", \"area_name\") \\\n",
    "    .agg(count(\"*\").alias(\"total_cases\"))\n",
    "\n",
    "cases_closed_df = cases_with_year_df.filter(\n",
    "    (~col(\"status_desc\").contains(\"UNK\")) & \n",
    "    (~col(\"status_desc\").contains(\"Invest Cont\")))\n",
    "\n",
    "cases_closed_by_station_year_df = cases_closed_df.groupBy(\"year\", \"area_name\") \\\n",
    "    .agg(count(\"*\") \\\n",
    "    .alias(\"closed_cases\"))\n",
    "\n",
    "cases_with_percentages_df = cases_closed_by_station_year_df.join(total_cases_by_station_year_df, [\"year\", \"area_name\"]) \\\n",
    "    .withColumn(\"closed_case_rate\",(col(\"closed_cases\") / col(\"total_cases\") * 100))\n",
    "\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(col(\"closed_case_rate\").desc())\n",
    "\n",
    "query2_results_df = cases_with_percentages_df.withColumn(\"rank\", dense_rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 3) \\\n",
    "    .select(\"year\", col(\"area_name\").alias(\"precinct\"), \"closed_case_rate\", \"rank\")\n",
    "\n",
    "query2_results_df = query2_results_df.orderBy(\"year\", \"rank\")\n",
    "\n",
    "query2_results_df.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7dde6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
